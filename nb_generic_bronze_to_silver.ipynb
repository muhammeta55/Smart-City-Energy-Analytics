{"cells":[{"cell_type":"code","source":["# nb_generic_bronze_to_silver\n","# Run config and functions"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cd435909-08b8-4617-883a-9fe73fa11fa8"},{"cell_type":"code","source":["%run ./nb_config\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"9ec994a7-7ca8-47dc-8e17-08a4477051b0","normalized_state":"finished","queued_time":"2026-01-04T17:43:09.5676709Z","session_start_time":"2026-01-04T17:43:09.5680729Z","execution_start_time":"2026-01-04T17:43:22.1428029Z","execution_finish_time":"2026-01-04T17:43:22.1988328Z","parent_msg_id":"6e50007c-a3f3-4a2a-829b-3ba20152535a"},"text/plain":"StatementMeta(, 9ec994a7-7ca8-47dc-8e17-08a4477051b0, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Config Loaded Successfully\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"eb3e6a5e-a5d7-4735-b5a2-b0362cc279da"},{"cell_type":"code","source":["%run ./nb_logging\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[4,5],"state":"finished","livy_statement_state":"available","session_id":"9ec994a7-7ca8-47dc-8e17-08a4477051b0","normalized_state":"finished","queued_time":"2026-01-04T17:43:23.7882207Z","session_start_time":null,"execution_start_time":"2026-01-04T17:43:25.2301273Z","execution_finish_time":"2026-01-04T17:43:25.3101978Z","parent_msg_id":"47c86528-66ea-441c-9955-8a9c3fc7c4ee"},"text/plain":"StatementMeta(, 9ec994a7-7ca8-47dc-8e17-08a4477051b0, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2026-01-04 17:43:25 - nb_logging - INFO - Logging notebook initialized successfully.\n"]}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6e546ba4-7296-4ef7-81e1-75046939450c"},{"cell_type":"code","source":["%run ./nb_functions"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[6,7,8],"state":"finished","livy_statement_state":"available","session_id":"9ec994a7-7ca8-47dc-8e17-08a4477051b0","normalized_state":"finished","queued_time":"2026-01-04T17:43:30.9338201Z","session_start_time":null,"execution_start_time":"2026-01-04T17:43:32.0643195Z","execution_finish_time":"2026-01-04T17:43:32.1392909Z","parent_msg_id":"5ca8d876-93f4-4f25-8dbf-5e9592c3f56f"},"text/plain":"StatementMeta(, 9ec994a7-7ca8-47dc-8e17-08a4477051b0, 8, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2cb26ca3-62fc-4203-9d27-0688697c55c4"},{"cell_type":"code","source":["logger = get_logger(__name__)\n","\n","dataframe_collection = {}\n","step_results = {}\n","\n","# Block 2: Extract\n","current_stage = \"extracted\"\n","dataframe_collection[current_stage] = {}\n","total_count = 0\n","\n","for source, configs in source_configs.items():\n","    src_path = f\"{LKHS_PATH_SRC}{configs['source']['src_directory']}\"\n","    if not is_directory_empty(src_path):\n","        df = spark.read.format(FILE_FORMAT).load(src_path)\n","        step_results[f\"Extraction_{source}\"] = \"SUCCESS\"\n","        count = df.count()\n","        if count > 0:\n","            dataframe_collection[current_stage][source] = df\n","            total_count += count\n","\n","if total_count == 0:\n","    mssparkutils.notebook.exit(\"All source dataframes are empty.\")\n","\n","# Block 3a: Simplified\n","previous_stage = \"extracted\"\n","current_stage = \"simplified\"\n","dataframe_collection[current_stage] = {}\n","\n","for source, data in dataframe_collection[previous_stage].items():\n","    configs = source_configs[source]\n","    if 'simplify_structure' in configs:\n","        for s in configs['simplify_structure']:\n","            if s[\"method\"] == \"explode\": data = explode_col(data, s[\"col\"])\n","            elif s[\"method\"] == \"flatten\": data = flatten_struct_col(data, s[\"col\"])\n","            elif s[\"method\"] == \"zip_explode\": data = zip_explode_and_flatten(data, s[\"parent\"], s[\"children\"])\n","    dataframe_collection[current_stage][source] = data\n","step_results[\"Transformation_Simplified\"] = \"SUCCESS\"\n","\n","# Block 3b/c: Datatyped, Standardized & Normalized\n","\n","dataframe_collection[\"standardized\"] = {}\n","\n","for k, v in dataframe_collection[\"simplified\"].items():\n","    # 1. Run general cleaning (column names to snake_case)\n","    df_standard = clean_column_names_and_schemas(v)\n","    \n","    # 2. Specific Alignment for Energy: Rename to 'time'\n","    if k == \"energy_config\":\n","        if \"readingdate\" in df_standard.columns:\n","            df_standard = df_standard.withColumnRenamed(\"readingdate\", \"time\")\n","        elif \"reading_date\" in df_standard.columns:\n","            df_standard = df_standard.withColumnRenamed(\"reading_date\", \"time\")\n","            \n","    # 3. DATA TYPE NORMALIZATION: Force 'time' to Timestamp\n","    # This prevents the NULL join issue in Gold\n","    if \"time\" in df_standard.columns:\n","        df_standard = df_standard.withColumn(\"time\", to_timestamp(col(\"time\")))\n","            \n","    dataframe_collection[\"standardized\"][k] = df_standard\n","\n","step_results[\"Transformation_Standardized\"] = \"SUCCESS\"\n","    \n","# Block 3d: Meta (Primary Key logic exactly same)\n","current_stage = \"meta\"\n","dataframe_collection[current_stage] = {}\n","for source, data in dataframe_collection[\"standardized\"].items():\n","    configs = source_configs[source]\n","    if 'primary_col' in configs:\n","        pk_cfg = configs['primary_col']\n","        cols = [col(c).cast(\"string\") for c in pk_cfg[\"lokaal_id_cols\"]]\n","        data = data.withColumn(\"lokaal_id_col\", concat_ws(\"_\", *cols))\n","        data = data.withColumn(pk_cfg[\"primary_col_name\"], hash_column(lit(pk_cfg[\"objecttype\"]), lit(_SOURCE), col(\"lokaal_id_col\")))\n","    \n","    data = data.withColumn(SOURCE_COL_NAME, lit(_SOURCE))\n","    data = data.withColumn(IMPORTDATE_COL_NAME, current_timestamp())\n","    dataframe_collection[current_stage][source] = data\n","step_results[\"Transformation_Meta\"] = \"SUCCESS\"\n","\n","# Block 3e: Selected\n","dataframe_collection[\"selected\"] = {}\n","for source, data in dataframe_collection[\"meta\"].items():\n","    if 'columns_to_select' in source_configs[source]:\n","        data = select_columns_safe(data, source_configs[source]['columns_to_select'])\n","    dataframe_collection[\"selected\"][source] = data\n","\n","# Block 4: Load & Create Table\n","spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n","dataframe_collection[\"final\"] = {}\n","for source, data in dataframe_collection[\"selected\"].items():\n","    configs = source_configs[source]\n","    sink_path = f\"{LKHS_PATH_DES}{configs['sink']['sink_directory']}\"\n","    load_data_into_delta_table(data, sink_path, FULL_LOAD, configs[\"primary_col\"][\"primary_col_name\"])\n","    create_lakehouse_table(f\"{_SOURCE}_{configs['sink']['sink_directory']}\", sink_path)\n","    step_results[f\"Load_to_Delta_{source}\"] = f\"SUCCESS ({data.count()} records)\"\n","\n","# --- ADD THIS NEW BLOCK AT THE END ---\n","print(\"\\n\" + \"=\"*50)\n","print(\"ETL STEP EXECUTION REPORT\")\n","print(\"=\"*50)\n","for step, status in step_results.items():\n","    print(f\"STEP: {step.ljust(30)} | STATUS: {status}\")\n","print(\"=\"*50)\n","\n","# Debug Check: If a step is missing from this list, you know exactly where the code stopped."],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"9ec994a7-7ca8-47dc-8e17-08a4477051b0","normalized_state":"finished","queued_time":"2026-01-04T17:47:52.0174996Z","session_start_time":null,"execution_start_time":"2026-01-04T17:47:52.0187168Z","execution_finish_time":"2026-01-04T17:48:08.8923075Z","parent_msg_id":"78d5aa14-c70c-439e-aee3-3855d83b0448"},"text/plain":"StatementMeta(, 9ec994a7-7ca8-47dc-8e17-08a4477051b0, 11, Finished, Available, Finished)"},"metadata":{}},{"output_type":"error","ename":"UnsupportedOperationException","evalue":"[DELTA_MULTIPLE_SOURCE_ROW_MATCHING_TARGET_ROW_IN_MERGE] Cannot perform Merge as multiple source rows matched and attempted to modify the same\ntarget row in the Delta table in possibly conflicting ways. By SQL semantics of Merge,\nwhen multiple source rows match on the same target row, the result may be ambiguous\nas it is unclear which source row should be used to update or delete the matching\ntarget row. You can preprocess the source table to eliminate the possibility of\nmultiple matches. Please refer to\nhttps://docs.delta.io/latest/delta-update.html#upsert-into-a-table-using-merge","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnsupportedOperationException\u001b[0m             Traceback (most recent call last)","Cell \u001b[0;32mIn[32], line 92\u001b[0m\n\u001b[1;32m     90\u001b[0m configs \u001b[38;5;241m=\u001b[39m source_configs[source]\n\u001b[1;32m     91\u001b[0m sink_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLKHS_PATH_DES\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mconfigs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msink\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msink_directory\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 92\u001b[0m load_data_into_delta_table(data, sink_path, FULL_LOAD, configs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprimary_col\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprimary_col_name\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     93\u001b[0m create_lakehouse_table(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_SOURCE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfigs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msink\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msink_directory\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, sink_path)\n\u001b[1;32m     94\u001b[0m step_results[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoad_to_Delta_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSUCCESS (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m records)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n","Cell \u001b[0;32mIn[23], line 36\u001b[0m, in \u001b[0;36mload_data_into_delta_table\u001b[0;34m(data, sink_path, full_load, primary_col_name)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     dt \u001b[38;5;241m=\u001b[39m DeltaTable\u001b[38;5;241m.\u001b[39mforPath(spark, sink_path)\n\u001b[1;32m     35\u001b[0m     dt\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmerge(data\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprimary_col_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m = s.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprimary_col_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m---> 36\u001b[0m       \u001b[38;5;241m.\u001b[39mwhenMatchedUpdateAll()\u001b[38;5;241m.\u001b[39mwhenNotMatchedInsertAll()\u001b[38;5;241m.\u001b[39mexecute()\n","File \u001b[0;32m/usr/hdp/current/spark3-client/jars/delta-spark_2.12-3.2.1.20251125.5.jar/delta/tables.py:1065\u001b[0m, in \u001b[0;36mDeltaMergeBuilder.execute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;241m0.4\u001b[39m)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;124;03m    Execute the merge operation based on the built matched and not matched actions.\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m \n\u001b[1;32m   1063\u001b[0m \u001b[38;5;124;03m    See :py:class:`~delta.tables.DeltaMergeBuilder` for complete usage details.\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1065\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jbuilder\u001b[38;5;241m.\u001b[39mexecute()\n","File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n","\u001b[0;31mUnsupportedOperationException\u001b[0m: [DELTA_MULTIPLE_SOURCE_ROW_MATCHING_TARGET_ROW_IN_MERGE] Cannot perform Merge as multiple source rows matched and attempted to modify the same\ntarget row in the Delta table in possibly conflicting ways. By SQL semantics of Merge,\nwhen multiple source rows match on the same target row, the result may be ambiguous\nas it is unclear which source row should be used to update or delete the matching\ntarget row. You can preprocess the source table to eliminate the possibility of\nmultiple matches. Please refer to\nhttps://docs.delta.io/latest/delta-update.html#upsert-into-a-table-using-merge"]}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3181aa70-53cb-45fd-a62f-b9baf87c732f"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"880a2fb2-8672-4c97-a3a0-56f067dc2207","known_lakehouses":[{"id":"880a2fb2-8672-4c97-a3a0-56f067dc2207"}],"default_lakehouse_name":"smart_city_lakehouse","default_lakehouse_workspace_id":"ce7fad14-2d0e-40e4-8c22-a560e4ffafd3"}}},"nbformat":4,"nbformat_minor":5}