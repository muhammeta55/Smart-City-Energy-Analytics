{"cells":[{"cell_type":"code","source":["# nb_functions"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"10f513be-fbed-4816-9383-0ec5eeaf9481"},{"cell_type":"code","source":["from pyspark.sql.types import StructType, ArrayType\n","import hashlib\n","from pyspark.sql.functions import *\n","from delta.tables import DeltaTable\n","from pyspark.sql.functions import to_timestamp, col"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":[],"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2025-12-30T19:40:01.2090838Z","session_start_time":"2025-12-30T19:40:01.2100687Z","execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"64f58223-1c61-4b88-8a3a-a6ab194dafb9"},"text/plain":"StatementMeta(, , -1, Waiting, , Waiting)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0e22e7df-6431-4527-8871-4be8bf426fc6"},{"cell_type":"code","source":["# --- File System Helpers ---\n","def is_directory_empty(path):\n","    try: return len(mssparkutils.fs.ls(path)) == 0\n","    except: return True\n","\n","# --- Transformation Helpers ---\n","def explode_col(df, column): \n","    return df.withColumn(column, explode(col(column)))\n","\n","def flatten_struct_col(df, column): \n","    return df.select(\"*\", f\"{column}.*\").drop(column)\n","\n","def zip_explode_and_flatten(df, parent, children):\n","    zipped = arrays_zip(*[col(f\"{parent}.{c}\") for c in children])\n","    df = df.withColumn(\"temp\", explode(zipped))\n","    for child in children:\n","        df = df.withColumn(child, col(f\"temp.{child}\"))\n","    return df.drop(\"temp\", parent)\n","\n","# --- Metadata & Security Helpers ---\n","def hash_column(objecttype_col, bron_col, lokaal_id_col):\n","    return md5(concat(objecttype_col, bron_col, lokaal_id_col))\n","\n","def select_columns_safe(df, columns):\n","    \"\"\"Prevents crashes if a column is missing in source.\"\"\"\n","    return df.select(*[c for c in columns if c in df.columns])\n","\n","def clean_column_names_and_schemas(df):\n","    \"\"\"Standardizes column names to snake_case.\"\"\"\n","    import re\n","    for col_name in df.columns:\n","        new_name = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', col_name).lower()\n","        df = df.withColumnRenamed(col_name, new_name)\n","    return df\n","\n","# --- üü¢ IMPROVED: Load & Merge Logic ---\n","def load_data_into_delta_table(data, sink_path, full_load, primary_col_name):\n","    \"\"\"\n","    Handles both initial creation and incremental Upserts (Merge).\n","    Added: check for path existence to prevent 'Path does not exist' errors.\n","    \"\"\"\n","    # 1. Check if table exists\n","    try:\n","        is_delta = DeltaTable.isDeltaTable(spark, sink_path)\n","    except:\n","        is_delta = False\n","\n","    # 2. Load Logic\n","    if full_load or not is_delta:\n","        # Overwrite/Create\n","        data.write.format(\"delta\") \\\n","            .mode(\"overwrite\") \\\n","            .option(\"overwriteSchema\", \"true\") \\\n","            .save(sink_path)\n","    else:\n","        # Incremental Merge (Upsert)\n","        # ‚ö†Ô∏è CRITICAL: We deduplicate 'data' here to prevent the \n","        # MULTIPLE_SOURCE_ROW_MATCHING error we saw earlier.\n","        deduplicated_data = data.dropDuplicates([primary_col_name])\n","        \n","        dt = DeltaTable.forPath(spark, sink_path)\n","        dt.alias(\"t\").merge(\n","            deduplicated_data.alias(\"s\"), \n","            f\"t.{primary_col_name} = s.{primary_col_name}\"\n","        ).whenMatchedUpdateAll() \\\n","         .whenNotMatchedInsertAll() \\\n","         .execute()\n","\n","# --- Table Registration ---\n","def create_lakehouse_table(table_name, path):\n","    \"\"\"Registers the Delta files as a proper SQL table in the Lakehouse.\"\"\"\n","    spark.sql(f\"CREATE TABLE IF NOT EXISTS {table_name} USING DELTA LOCATION '{path}'\")\n","    spark.sql(f\"REFRESH TABLE {table_name}\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":[],"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2026-01-01T18:32:47.0859797Z","session_start_time":"2026-01-01T18:32:47.0870569Z","execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"09481bec-2bcc-4ca9-a533-1887c6320c48"},"text/plain":"StatementMeta(, , -1, Waiting, , Waiting)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e4704bdc-4031-48c8-afc4-d9b05d153777"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"880a2fb2-8672-4c97-a3a0-56f067dc2207","known_lakehouses":[{"id":"880a2fb2-8672-4c97-a3a0-56f067dc2207"}],"default_lakehouse_name":"smart_city_lakehouse","default_lakehouse_workspace_id":"ce7fad14-2d0e-40e4-8c22-a560e4ffafd3"}}},"nbformat":4,"nbformat_minor":5}